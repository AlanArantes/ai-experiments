{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# AI: How easy is it to create a helpdesk assistant using Transformers library\n",
        "\n",
        "*Imagine being the developer on your squad, crafting a bot that doesn’t just spit out automatic responses but actually gets what your users are asking. That’s the kind of magic we’re talking about here. And guess what? It’s not as complicated as it sounds, especially with this guide.*\n",
        "\n",
        "---\n",
        "\n",
        "First off, if you’re just starting with AI or have poked around a bit, you’ve probably heard of the OpenAI API and the Transformers library. Both are like having your own Gandalf for building AI stuff, but they cater to different kinds of wizards. OpenAI API is like the express train to AI-ville. It’s super user-friendly, meaning you don’t need to know the ins and outs of AI to make something cool. It’s perfect for getting your app to chat away or understand user commands without getting your hands dirty with the nitty-gritty of model training.\n",
        "\n",
        "## Here’s the game plan:\n",
        "Preparing the state: Start by installing the right dependencies to use the Transform library. Ensure you’ve got Python installed — most spells in the AI world are cast with it. If you haven’t, head over to the Python website and follow the instructions for your operating system.\n",
        "\n",
        "*** On the colab platform set Your Collab Runtime type to T4 GPU in the Menu: Runtime > Change runtime type > Hardware accelerator ***\n",
        "\n",
        "Next, open your command line or terminal and cast the following spell to summon the Transformers library into your realm (or just install it):"
      ],
      "metadata": {
        "id": "KLVzRlziMHNO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NdYXmvaFK3pF"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Choose Your Wand** — CPU or GPU?: Here’s where you pick the power source for your magic. If you’re just tinkering or learning, running the models on your CPU might be fine. But, if you’re planning to do some heavy spellwork or want super-fast responses, you’ll want to harness the power of a GPU. In this way consider to additional dependencies:"
      ],
      "metadata": {
        "id": "7k7bl1icM7Dc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate -U"
      ],
      "metadata": {
        "id": "ygF_COfHM2mG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*On the colab platform you can skip this step*\n",
        "\n",
        "**NVIDIA CUDA for GPU Wizards:** If you’ve chosen the GPU path and have an NVIDIA graphics card, you’ll need to install CUDA and cuDNN to get everything running smoothly. These are like the arcane texts that let your GPU understand how to conjure AI spells. Head over to the NVIDIA website, find the versions compatible with your graphics card, and follow the installation guides.\n",
        "\n",
        "[NVidia CUDA Toolkit](https://developer.nvidia.com/cuda-toolkit)\n",
        "\n",
        "## Pick Your Potion: Choosing the right model.\n",
        "There are tons out there, but for a helpdesk assistant, you’ll want something good with text, like BERT or GPT-2. We’re going with GPT-2 in this tutorial."
      ],
      "metadata": {
        "id": "UEIPGJKfNBkc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2TokenizerFast, TextDataset, DataCollatorForLanguageModeling\n",
        "from transformers import Trainer, TrainingArguments, GPT2Config"
      ],
      "metadata": {
        "id": "DIrP7fkSNhV3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Montage\n",
        "Next up, you’ll need to train your model. This sounds epic because it is. You’re essentially teaching your bot to understand and respond to queries by feeding it examples. We need to consider having a starting dataset to teach the AI and witness the magic unfold. Below, there’s a small set of hypothetical Q&A to use in our tutorial, just copy them and save in a text file in your project folder, for example with the name dataset.txt\n",
        "\n",
        "```\n",
        "# My smartphone is not turning on. What should I do?\n",
        "# I'm sorry to hear that. Let's try a few troubleshooting steps. First, make sure the battery is charged. If it doesn't work, press and hold the power button for at least 10 seconds to force a restart.\n",
        "\n",
        "# How can I transfer photos from my phone to my computer?\n",
        "# To transfer photos, connect your phone to the computer using a USB cable. Once connected, open the file explorer on your computer, find your phone, and navigate to the \"DCIM\" folder. Copy the photos from there to your computer.\n",
        "\n",
        "# I forgot my phone's PIN. How can I unlock it?\n",
        "# No worries! If you've set up a Google or Apple account on your phone, you can use the \"Forgot PIN\" or \"Forgot Password\" option on the lock screen. Follow the on-screen instructions to reset your PIN.\n",
        "\n",
        "# How do I update the software on my smartphone?\n",
        "# To update your phone's software, go to Settings > System > Software Update. If there's an update available, follow the on-screen prompts to download and install it. Ensure your phone is connected to Wi-Fi during the update.\n",
        "\n",
        "# My phone's camera is blurry. What can I do to fix it?\n",
        "# Blurriness in the camera might be due to smudges on the lens. Gently clean the lens with a microfiber cloth. If the issue persists, check if there are any software updates for the camera app in your phone's app store.\n",
        "\n",
        "# How can I set up email on my smartphone?\n",
        "# To set up email, go to Settings > Accounts > Add Account. Select \"Email\" and enter your email address and password. The phone will automatically configure the email settings. If you're having trouble, double-check your email provider's settings.\n",
        "\n",
        "# My phone is running slow. Any tips to speed it up?\n",
        "# Slow performance can be due to multiple apps running in the background. Close unused apps and clear cache from Settings > Storage > Cached data. If the issue persists, consider uninstalling unnecessary apps or freeing up storage space.\n",
        "\n",
        "# How can I disable notifications for a specific app?\n",
        "# To disable notifications for a specific app, go to Settings > Apps > [App Name]. Look for the \"Notifications\" section and toggle off the switch. This will stop notifications from that app.\n",
        "\n",
        "# My phone's screen is cracked. What should I do?\n",
        "# I'm sorry to hear that. If the screen is still functional, you may consider using a screen protector temporarily. For a permanent solution, contact the phone manufacturer or visit an authorized service center for screen repair or replacement.\n",
        "\n",
        "# How do I factory reset my smartphone?\n",
        "# Performing a factory reset will erase all data on your phone. To do this, go to Settings > System > Reset > Factory data reset. Follow the on-screen instructions and ensure you have backed up important data before proceeding.\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ehShfxSXNoww"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get your mind ready\n",
        "We’re going through a recipe for some digital wizardry, specifically for training an AI to be your next-gen helpdesk assistant.\n",
        "\n",
        "Kick-off with the Tokenizer: Imagine you’ve got a blender (that’s your tokenizer), and you’re throwing in some text to chop up into bits that your AI can easily digest. We’re using the GPT2TokenizerFast here, tuned to ‘gpt2’, which means it’s ready to handle text the way GPT-2 likes it."
      ],
      "metadata": {
        "id": "BCBKB6p9Ob45"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a tokenizer\n",
        "tokenizer = GPT2TokenizerFast.from_pretrained('gpt2')"
      ],
      "metadata": {
        "id": "L23n3PTLOgEj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Whip Up the Dataset:** Now, think of the dataset as your main ingredient. This line tells your setup, “Hey, grab the helpdesk conversations from this text file.” It’s like you’re prepping your veggies before you start cooking. The block_size=128 bit is about how big a bite you want your AI to chew on in one go. Here, consider changing the file_path attribute to point to your previously created dataset."
      ],
      "metadata": {
        "id": "RhAxIXQZOlbq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the dataset\n",
        "# Using Google Drive as a storage for your dataset\n",
        "# from google.colab import drive\n",
        "# drive.mount(\"/content/drive\")\n",
        "# dataset = TextDataset(tokenizer=tokenizer, file_path=\"./drive/My Drive/Colab Notebooks/dataset.txt\", block_size=128)\n",
        "dataset = TextDataset(tokenizer=tokenizer, file_path=\"./dataset.txt\", block_size=128)"
      ],
      "metadata": {
        "id": "lwNs1LceOtjD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Collator — The Mixer:** This part’s all about getting your data ready for the oven. The DataCollatorForLanguageModeling is like saying, “Mix up my text chunks so they’re just right for training.” Setting mlm=False tells it we’re not hiding any of the text from our model; it gets to see everything full on."
      ],
      "metadata": {
        "id": "3BVIIKVSPF4E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the data collator\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
      ],
      "metadata": {
        "id": "RyIv8hNAPJOv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Time — Preheating the Oven:** Here’s where you pick your oven model. We’re rolling with GPT-2, a pretty fancy choice if you’re into making your AI understand and generate human-like text. We skip the part where we build the oven from scratch (GPT2Config) and go straight to using a preheated one with GPT2LMHeadModel.from_pretrained(‘gpt2’)."
      ],
      "metadata": {
        "id": "IzY3w0mGPMze"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a blank model with GPT-2 architecture\n",
        "# config = GPT2Config.from_pretrained('gpt2')\n",
        "# model = GPT2LMHeadModel(config)\n",
        "# Load the pre-trained GPT-2 model\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')"
      ],
      "metadata": {
        "id": "WFF3v2jv257v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training Arguments — Setting the Timer:** This chunk is like dialing in the settings on your oven. You’re telling it how long to cook (num_train_epochs=100, which is like saying, “Let’s run this 100 times to make sure it’s really got it”), how hot (or how many examples to look at once with per_device_train_batch_size=32), and when to check if the dish is done (save_steps=10_000). Oh, and save_total_limit=2 is kinda like saying, “I’ve only got space for two leftovers in the fridge, so toss anything older.”"
      ],
      "metadata": {
        "id": "A2AwZj71PUXC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./gpt2_pretrained\",  # The output directory\n",
        "    overwrite_output_dir=True,  # Overwrite the content of the output directory\n",
        "    num_train_epochs=100,  # Number of training epochs\n",
        "    per_device_train_batch_size=32,  # Batch size for training\n",
        "    save_steps=10_000,  # Number of updates steps before two checkpoint saves\n",
        "    save_total_limit=2,  # Limit the total amount of checkpoints and delete the older checkpoints\n",
        ")"
      ],
      "metadata": {
        "id": "1pumnuQrPYdT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Start the Training:** With everything in place, you kick off the training. This is when your AI starts learning from the data, making sense of the patterns, understanding questions, and figuring out how to generate answers. Imagine it’s like baking a cake; you’ve set the temperature and time, now you let it transform inside the oven. The training process is where the magic happens, turning your raw data into a sophisticated AI model capable of handling real-world queries. It’s a mix of art and science, requiring patience and adjustments in the dataset or on the training arguments to get it just right.\n",
        "\n",
        "*On the colab platform don't forget to set your runtime type to T4 GPU*"
      ],
      "metadata": {
        "id": "mMdXhYIyPb7l"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_B7PPxCWpulF"
      },
      "outputs": [],
      "source": [
        "# Initialize the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=dataset,\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Save the model\n",
        "trainer.save_model()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Putting all this together, you’re setting up a state-of-the-art kitchen to whip up an AI that’s gonna handle your helpdesk like a pro. It’s all about prepping your ingredients, mixing them right, and cooking them under the perfect conditions. Bon appétit, tech-style!\n",
        "\n",
        "## Using your brain\n",
        "We’re gonna walk through whipping up a batch of smart, on-the-fly responses using some serious coding chops and AI wizardry. Let’s get into the nitty-gritty of this tech recipe.\n",
        "\n",
        "Gathering the Ingredients: We need make some imports before we start to use our brain previously stored."
      ],
      "metadata": {
        "id": "rC5IciutQXXd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "from time import time\n",
        "import torch\n",
        "\n",
        "from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n",
        "\n",
        "from functools import lru_cache"
      ],
      "metadata": {
        "id": "l0yc8jyo8k5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important notes:** We’re calling in the big guns from the Transformers library: the GPT-2 LM Head Model and GPT-2 Tokenizer Fast. Think of them as your high-end food processor and slicer, ready to handle anything you throw at them. To enhance performance and avoid redundant calls that yield the same result, we’ve also decided to implement a small cache layer using the @lru_cache annotation.\n",
        "\n",
        "Continuing with our activities, we initialize our tokenizer, which is like setting up your cutting board and knives to chop up text into manageable pieces our AI can understand."
      ],
      "metadata": {
        "id": "_pvvJoTf8oxi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a tokenizer\n",
        "tokenizer = GPT2TokenizerFast.from_pretrained('gpt2')"
      ],
      "metadata": {
        "id": "M4WrgGyr8sto"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We load our pre-trained model, kind of like getting that secret recipe book out. We’re setting up to use either the GPU or CPU, depending on what kind of firepower we have available. This is the equivalent of deciding if you’re cooking on a gas stove or an electric one."
      ],
      "metadata": {
        "id": "oyNT924O8wPz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the trained model for inference\n",
        "model = GPT2LMHeadModel.from_pretrained(\"./gpt2_pretrained\")\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"device: {device}\")\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "OcOfcUT68zlz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Getting Down to Business:** We briefly consider making our operations more efficient with model.half() to use FP16 precision, kind of like choosing to prep your ingredients beforehand. But we decide to leave that for another day and keep things straightforward.\n",
        "\n",
        "Now, here’s where we start cooking for real. We define a generate_response function, which is our recipe for turning questions into answers. This function is like a magic cauldron that takes in a question, stirs it through the AI model, and poof! Out comes a response.\n",
        "\n",
        "We’re not just blindly cooking here; we’re timing each step of the process — from slicing and dicing the question to simmering it in the AI model and serving up the response. It’s all about efficiency in our kitchen.\n",
        "\n",
        "When we generate the response, we’re setting some rules like how long we want our answer to be (max_length=50) and making sure it's original (no_repeat_ngram_size=2). It's like seasoning to taste and making sure not to overcook."
      ],
      "metadata": {
        "id": "bcndJZoI80Od"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use FP16 precision\n",
        "# model = model.half()\n",
        "\n",
        "# Define a function for generating responses\n",
        "@lru_cache(maxsize=None)\n",
        "def generate_response(input_text):\n",
        "  start_tokenizer = time()\n",
        "  inputs = tokenizer.encode(input_text, return_tensors='pt')\n",
        "  inputs = inputs.to(device)\n",
        "  stop_tokenizer = time()\n",
        "\n",
        "  start_generate = time()\n",
        "  outputs = model.generate(inputs, max_length=50, num_return_sequences=1, no_repeat_ngram_size=2, temperature=0.7)\n",
        "  stop_generate = time()\n",
        "\n",
        "  start_decode = time()\n",
        "  full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "  stop_decode = time()\n",
        "\n",
        "  start_find_response = time()\n",
        "  guard = full_response.find('User:') if full_response.find('User:') > 0 else len(full_response)\n",
        "  guard -= len('User:')\n",
        "  response = full_response[0:guard].strip()\n",
        "  stop_find_response = time()\n",
        "\n",
        "  print(f\"tokenizer elapsed time {stop_tokenizer - start_tokenizer}\")\n",
        "  print(f\"generate elapsed time {stop_generate - start_generate}\")\n",
        "  print(f\"decode elapsed time {stop_decode - start_decode}\")\n",
        "  print(f\"find_response elapsed time {stop_find_response - start_find_response}\")\n",
        "  return response"
      ],
      "metadata": {
        "id": "4rP32cef872j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Serving the Dish:** After our AI assistant does its magic, we present the magic — I mean, the answer. We even clean it up a bit, trimming off any excess to make sure it’s just what the user asked for. Finally, we take our test question about a blurry phone camera."
      ],
      "metadata": {
        "id": "5nU3bLJF894C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "27SjMJ61ZpUY"
      },
      "outputs": [],
      "source": [
        "# Use this function in your command-line interface to generate responses to user input\n",
        "input = \"My phone's camera is blurry. What can I do to fix it?\"\n",
        "print(f\"input {input}\")\n",
        "print(generate_response(input))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMCIml42tADcoTc1NhOI3AF"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}